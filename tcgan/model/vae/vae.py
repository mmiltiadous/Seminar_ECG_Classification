"""
Codebase of Convolutional variational autoencoder

"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from time import time
import pandas as pd
import matplotlib.pyplot as plt

from mlpy.lib.utils.config import MLModelConfig

from ...lib.vis import save_series
from ...lib.ops import conv_out_size_same, output_padding, tensor2numpy, get_log_str


class CVAEConfig(MLModelConfig):
    def __init__(self,
                 input_shape,
                 log_dir,
                 logger,
                 latent_dim=100,
                 kernel_size=5,
                 strides=2,
                 padding='same',
                 batch_size=16,
                 epochs=300,
                 lr=1e-4,
                 n_examples_to_generate=16,
                 seed=42,
                 verbose=0,
                 **kwargs
                 ):
        super().__init__(log_dir, logger, seed=seed, verbose=verbose, **kwargs)

        self.input_shape = input_shape
        self.latent_dim = latent_dim
        self.kernel_size = kernel_size
        self.strides = strides
        self.padding = padding
        self.batch_size = batch_size
        self.epochs = epochs
        self.lr = lr
        self.n_examples_to_generate = n_examples_to_generate


class CVAEModel(tf.keras.Model):
    def __init__(self, cfg):
        super(CVAEModel, self).__init__()
        self.cfg = cfg
        self._build_model()

    def _build_model(self):
        """
        Note, it's common practice to avoid using batch normalization when training VAEs, since the additional
        stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling.
        """
        input_shape = self.cfg.input_shape
        kernel_size = self.cfg.kernel_size
        strides = self.cfg.strides
        padding = self.cfg.padding
        latent_dim = self.cfg.latent_dim

        # encoder
        self.encoder = tf.keras.Sequential(
            [
                layers.InputLayer(input_shape=input_shape),
                layers.Conv1D(32, kernel_size, strides, padding, activation='relu'),
                layers.Conv1D(64, kernel_size, strides, padding, activation='relu'),
                layers.Flatten(),
                # No activation
                layers.Dense(latent_dim + latent_dim),  # mean, logvar
            ]
        )

        # decoder
        d_layers = 3
        conv_units = [64, 32, 1]
        steps = input_shape[0]
        layer_steps = [steps]
        for i in range(d_layers):
            layer_steps.append(conv_out_size_same(layer_steps[-1], strides))
        layer_steps.reverse()

        self.decoder = tf.keras.Sequential()
        self.decoder.add(layers.InputLayer(input_shape=(latent_dim,)))
        self.decoder.add(layers.Dense(layer_steps[0] * 32, activation=tf.nn.relu))
        self.decoder.add(layers.Reshape((layer_steps[0], 32)))
        assert self.decoder.output_shape[1] == layer_steps[0]

        for i in range(d_layers):
            if layer_steps[i] * strides == layer_steps[i + 1]:
                self.decoder.add(layers.Conv1DTranspose(conv_units[i], kernel_size, strides, padding))
            else:
                self.decoder.add(layers.Conv1DTranspose(conv_units[i], kernel_size, strides, padding,
                                                        output_padding=output_padding(kernel_size, strides)))
            # I tried to use BatchNorm here on '50words' dataset and get a worse performance.
            # The experimental result is consistent with the tutorial description.
            # self.decoder.add(layers.BatchNormalization())
            # self.decoder.add(layers.LeakyReLU())
            assert self.decoder.output_shape[1] == layer_steps[i + 1]
        assert self.decoder.output_shape[-1] == input_shape[-1]

    def encode(self, x):
        """p(z|x)
        simply model the distribution as a diagonal Gaussian, and the network outputs the mean and log-variance
        parameters of a factorized Gaussian.
        Output log-variance instead of the variance directly for numerical stability.
        """
        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
        return mean, logvar

    def reparameterize(self, mean, logvar):
        """
        a trick to address the problem:
            The sampling operation creates a bottleneck because backpropagation cannot flow through a random node.
        z = mu + sigma * eps
            The latent variable  is now generated by a function of mu, sigma and eps, which would enable the model to
            backpropagate gradients in the encoder through mu and sigma respectively, while maintaining stochasticity
            through .
        """
        eps = tf.random.normal(shape=mean.shape)
        return eps * tf.exp(logvar * .5) + mean

    def decode(self, z, apply_sigmoid=False):
        """p(x|z)"""
        logits = self.decoder(z)
        if apply_sigmoid:
            probs = tf.sigmoid(logits)
            return probs
        return logits

    def call(self, x, training=None, mask=None):
        mean, logvar = self.encode(x)
        z = self.reparameterize(mean, logvar)
        x_hat = self.decode(z)
        return x_hat


class CVAE(object):
    def __init__(self, cfg):
        self.cfg = cfg
        self._build_model()

    def _build_model(self):
        self.model = CVAEModel(self.cfg)
        self.optimizer = tf.keras.optimizers.Adam(self.cfg.lr)

    def save(self):
        self.model.save_weights(os.path.join(self.cfg.ckpt_dir, 'model'))

    def load(self):
        self.model.load_weights(os.path.join(self.cfg.ckpt_dir, 'model'))

    def fit(self, data, test_data=None, restore=False):
        batch_size = self.cfg.batch_size
        epochs = self.cfg.epochs

        dataset_tr = tf.data.Dataset.from_tensor_slices(data).\
            shuffle(batch_size, reshuffle_each_iteration=True).batch(batch_size)

        res_records_tr = []
        res_records_te = []
        for epoch in range(1, epochs + 1):
            t_start = time()

            _res_list = []
            for train_x in dataset_tr:
                _res = self.train_step(train_x)
                _res_list.append(_res)
            _res_list_np = [tensor2numpy(r) for r in _res_list]
            _res_df = pd.DataFrame.from_records(_res_list_np)
            res = _res_df.mean().to_dict()
            log_str = get_log_str(res, epoch, self.cfg.epochs)
            log_str += f", time={(time() - t_start):.4}"
            self.cfg.logger.info(log_str)
            res.update({'epoch': epoch})
            res_records_tr.append(res)

            res = self.compute_loss(self.model, test_data)
            res = tensor2numpy(res)
            log_str = get_log_str(res, epoch, self.cfg.epochs)
            self.cfg.logger.info("eval on test set:" + log_str)
            res.update({'epoch': epoch})
            res_records_te.append(res)

        df_tr = pd.DataFrame.from_records(res_records_tr)
        df_te = pd.DataFrame.from_records(res_records_te)
        self.save_results(test_data, df_tr, df_te)
        self.save()

    def save_results(self, x_te, df_tr, df_te):
        df_tr.to_csv(os.path.join(self.cfg.train_dir, 'loss_tr.csv'))
        df_tr.to_csv(os.path.join(self.cfg.train_dir, 'loss_te.csv'))
        plt.figure()
        plt.plot(df_tr['loss'], label='loss_tr')
        plt.plot(df_te['loss'], label='loss_te')
        plt.legend(loc='best')
        plt.tight_layout()
        plt.savefig(os.path.join(self.cfg.train_dir, 'loss.png'))
        plt.close()

        decoded_samples = self.model(x_te)
        save_series(x_te, os.path.join(self.cfg.eval_dir, 'real_sample.png'))
        save_series(decoded_samples, os.path.join(self.cfg.eval_dir, 'decoded_sample.png'))

    @tf.function
    def train_step(self, x):
        with tf.GradientTape() as tape:
            loss_dict = self.compute_loss(self.model, x)
            loss = loss_dict['loss']
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
        return loss_dict

    @staticmethod
    def log_normal_pdf(sample, mean, logvar, raxis):
        log2pi = tf.math.log(2. * np.pi)
        return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)

    def compute_loss(self, model, x):
        """AEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:
        In practice, optimize the single sample Monte Carlo estimate of this expectation:
            log[p(x|z)] + log[p(z)] - log[q(z|x)]
        """
        mean, logvar = model.encode(x)
        z = model.reparameterize(mean, logvar)
        x_logit = model.decode(z)
        error = tf.metrics.mean_squared_error(x, x_logit)
        axis = tf.range(1, tf.size(error.shape))

        logpx_z = tf.reduce_mean(-tf.reduce_sum(error, axis=axis))
        logpz = tf.reduce_mean(self.log_normal_pdf(z, 0., 0., raxis=axis))
        logqz_x = tf.reduce_mean(self.log_normal_pdf(z, mean, logvar, raxis=axis))
        loss = -(logpx_z + logpz - logqz_x)

        res = {
            'loss': loss,
            'logpx_z': logpx_z,
            'logpz': logpz,
            'logqz_x': logqz_x
        }

        return res


